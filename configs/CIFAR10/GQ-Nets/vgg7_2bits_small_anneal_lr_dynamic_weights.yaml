__BASE__: configs/CIFAR10/GQ-Nets/vgg7_2bits.yaml
BATCH_SIZE_PER_GPU: &bs 128

## Dataset
data:
  train_loader_conf:
    batch_size: *bs
  train_sampler_conf:
    batch_size: *bs
  val_loader_conf:
    batch_size: *bs

## Optimization
param_group:
  conf:
    - type: Adam
      args: &weight_group
        lr: !!float 4e-4
        weight_decay: !!float 1e-4
      schedules:
        - name: &weight_lr_schedule1 weight_lr_annealing
          type: CosineAnnealingLR
          args:
            T_max: 50782 # number of iterations of final 130 epochs with 128 batch size
    - type: Adam
      args: &quant_param_group
        lr: !!float 1e-4
        weight_decay: 0.0
  groups:
    - <<: *weight_group
    - <<: *quant_param_group
  args:
    ft_layers: null
opt:
  args:
    alter_step: null  # jointly optimize W and Theta
schedule:
  variable_cfgs:
    - ["soft_w", "soft_loss"]
    - ["hard_w", "hard_loss"]
    - ["ref_w", 0]
  opt_cfgs:
    - [*weight_lr_schedule1, 170, 300, "iter"]

## Resume and snapshot
ckpt:
  dir: /mnt/lustre/lirundong/Data/quant-prob/cifar10/vgg7/2bits_small_lr_dynamic_weight/checkpoints/

## Logging and diagnose
log:
  tb_dir: /mnt/lustre/lirundong/Data/quant-prob/cifar10/vgg7/2bits_small_lr_dynamic_weight/tb_logs/
  file: /mnt/lustre/lirundong/Data/quant-prob/cifar10/vgg7/2bits_small_lr_dynamic_weight/train
