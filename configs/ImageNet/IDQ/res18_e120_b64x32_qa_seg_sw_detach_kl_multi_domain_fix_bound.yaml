## Common args
BATCH_SIZE_PER_GPU: &bs 64
TOTAL_EPOCHS: &e 120

## Dataset
data:
  dataset:
    type: ImageNetST
    args:
      img_dir: /mnt/lustre/share/images/
      meta_dir: /mnt/lustre/share/images/meta/
      color: true
  train_loader_conf:
    batch_size: *bs
    shuffle: false
    pin_memory: true
    num_workers: 2
  train_sampler_conf:
    batch_size: *bs
    total_epoch: *e
  val_loader_conf:
    batch_size: *bs
    shuffle: false
    pin_memory: true
    num_workers: 1

## Strategy
distil:
  mode: inv_distil # {null, inv_distil, distil}
  alter_step: &alter_n -1
  zero_momentum: false

quant:
  bitwidth:
    w: &qw 4
    a: &qa 4
  calib:
    steps: 5
    gamma: !!float 0.999
    required_on_training: true
    update_bn: false
  enable_at: &enable_q segmented  # {#milestone, segmented, begin}
  enable_fp: true
  align_zero: &qz false  # TODO: correct gradients for align_zero=True
  fp_layers: &fp_layers null

## Models
arch:
  type: resnet18_idq
  args:
    kw: *qw
    ka: *qa
    fp_layers: *fp_layers
    align_zero: *qz
    use_ckpt: false
    use_multi_domain: true
  sync_bn: true
  gpu_per_model: 1

teacher_arch: null

## Evaluation
eval:
  freq: 600
  vis: false
  quant: true
  calibrate: false
  use_ema_stat: true

loss:
  type: InvDistilLoss
  args:
    soft_weight: null
    temperature: 1.0
    detach_ref: true
  topk: [1, 5]

## Optimization
param_group:
  conf:
    - type: SGD
      args: &sgd_args
        lr: 0.025
        momentum: 0.9
        weight_decay: !!float 1e-4
        nesterov: true
    - type: Adam
      args: &adam_args
        lr: 0.001
        weight_decay: 0.0
  groups:
    - <<: *sgd_args
    - <<: *adam_args
  args:
    ft_layers: null

opt:
  args:
    alter_step: *alter_n

schedule:
  args:
    milestones: [60, 90]
    gamma: 0.1
    batch_size: *bs
    warmup_epochs: 4
    warmup_lr: 0.2
    enable_quant_at: *enable_q
    scheduled_variables:
      # fields: name, init_value, target_value, warmup_start_epoch, warmup_done_epoch, terminate_epoch=-1
      # warmup_{...}_epoch is the index of `schedule.milestones`, setup to null means do not tune it
      # multiple schedule for one variable is acceptable, which will thus be segmented by `terminate_epoch`s
      - ["soft_w", 0.0, 1.0, 10, 15, 60]
      - ["soft_w", 0.0, 1.0, 65, 70, 90]
      - ["soft_w", 0.0, 1.0, 95, 100, -1]
      - ["hard_w", 1.0, 1.0, null, null, -1]
      - ["ref_w", 1.0, 1.0, null, null, -1]

## Resume and snapshot
ckpt:
  freq: 1000
  dir: /mnt/lustre/lirundong/Data/quant-prob/imgnet/res18/idq_e120_b64x32_qa_seg_sw_detach_kl_multi_domain_fix_bound/checkpoints/

resume:
  path: null
  load_opt: true
  load_scheduler: true

## Diagnose
log:
  freq: 100
  tb_dir: /mnt/lustre/lirundong/Data/quant-prob/imgnet/res18/idq_e120_b64x32_qa_seg_sw_detach_kl_multi_domain_fix_bound/tb_logs/
  file: /mnt/lustre/lirundong/Data/quant-prob/imgnet/res18/idq_e120_b64x32_qa_seg_sw_detach_kl_multi_domain_fix_bound/train

diagnose:
  enabled: false
  diagnoser:
    type: null
    args: {}
  tasks: []

## Misc
comment: "idq_e120_b64x32_qa_seg_sw_detach_kl_multi_domain_fix_bound"
progress_bar: false
